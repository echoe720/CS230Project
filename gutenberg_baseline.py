# -*- coding: utf-8 -*-
"""Gutenberg_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_tpvbU0I4TT3Er-gjrHu3gB--4d9ur-2
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn import feature_extraction, linear_model, model_selection, preprocessing
import tensorflow as tf

# Commented out IPython magic to ensure Python compatibility.
#this mounts your Google Drive to the Colab VM.
#from google.colab import drive
#drive.mount('/content/drive', force_remount=True)

# enter the foldername in the Shared Google Drive
#FOLDERNAME = 'Shared drives/CS 230 Project'
#assert FOLDERNAME is not None, "[!] Enter the foldername."

# now that we've mounted your Drive, this ensures that
# the Python interpreter of the Colab VM can load
# python files from within it.
import sys
#sys.path.append('/content/drive/{}'.format(FOLDERNAME))

# %cd /content/drive/$FOLDERNAME/

datapath = "datasets/Gutenberg/"
train_df = pd.read_csv(datapath + "training.csv")
validation_df = pd.read_csv(datapath + "validation.csv")
#test_df = pd.read_csv(datapath + "test.csv")
#test_df = test_df.head(20000)

def map_authors(data):
    authors = []
    author_mappings = {}
    for index, row in data.iterrows():
        name = row['Author']
        if name in authors:
            continue
        authors.append(name)
        
    authors.sort()
    for i in range(len(authors)):
        author_mappings[authors[i]] = i
    return author_mappings

def reverse_mapping(author_mapping):
    reverse = {}
    for key in author_mapping.keys():
        reverse[author_mapping[key]] = key
    return reverse

author_mapping = map_authors(test_df)
print(author_mapping)
reverse_mappings = reverse_mapping(author_mapping)

count_vectorizer = feature_extraction.text.CountVectorizer()
train_vectors = count_vectorizer.fit_transform(train_df['Text'])
#test_vectors = count_vectorizer.fit_transform(test_df["Text"])

validation_vectors = count_vectorizer.transform(validation_df["Text"])

def extract_features(data):
  features = np.zeros((len(data.index), 2))
  for index, row in data.iterrows():
      text = row['Text']
      features[index, 0] = len(text) / 5
      words = text.split()
      features[index, 1] = len(text) / len(words)
  return features

def get_mappings(data, author_mapping):
    authors = data['Author']
    mappings = [author_mapping[author] for author in authors]
    data['mappings'] = mappings
    return data['mappings']

#test_targets = get_mappings(test_df, author_mapping)
#test_vectors = extract_features(test_df)
#validation_vectors = extract_features(validation_df)
train_targets = get_mappings(train_df, author_mapping)
validation_targets = get_mappings(validation_df, author_mapping)

num_rows = len(test_df.index)

from keras.utils import to_categorical
#label_test = test_targets
label_train = train_targets
label_v = validation_targets
# one hot encode
#encoded_label_test = to_categorical(label_test)
encoded_label_train = to_categorical(label_train)
encoded_label_v = to_categorical(label_v)

#from keras.regularizers import l2

ann = tf.keras.models.Sequential()

#clf = linear_model.RidgeClassifier()
#scores = model_selection.cross_val_score(clf, train_vectors, train_targets, cv=3, scoring="f1")
#scores

#ann.add(tf.keras.layers.Dense(units=100,  activation='relu'))

ann.add(tf.keras.layers.Dense(units=100, activation='relu'))

ann.add(tf.keras.layers.Dense(units=100, activation='relu'))

ann.add(tf.keras.layers.Dense(units=100, activation='relu'))

ann.add(tf.keras.layers.Dense(50, activation='softmax'))

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

ann.fit(train_vectors.todense(), encoded_label_train, batch_size = 32, epochs = 5)

y_pred = ann.predict(validation_vectors.todense())
#y_pred = ???
y_pred

y_output = np.zeros_like(y_pred)
y_output[np.arange(len(y_pred)), y_pred.argmax(1)] = 1

from sklearn.metrics import confusion_matrix, accuracy_score

accuracy_score(encoded_label_v, y_output)

def output_probs(reverse_mappings, output):
    probs = {}
    for i in range(len(output[0])):
        probs[reverse_mappings[i]] = round(output[0][i], 3)
    return probs

str = "Tom he made a sign to me--kind of a little noise with his mouth--and we went creeping away on our hands and knees. When we was ten foot off Tom whispered to me, and wanted to tie Jim to the tree for fun. But I said no; he might wake and make a disturbance, and then they'd find out I warn't in. Then Tom said he hadn't got candles enough, and he would slip in the kitchen and get some more. I didn't want him to try. I said Jim might wake up and come. But Tom wanted to resk it; so we slid in there and got three candles, and Tom laid five cents on the table for pay. Then we got out, and I was in a sweat to get away; but nothing would do Tom but he must crawl to where Jim was, on his hands and knees, and play something on him. I waited, and it seemed a good while, everything was so still and lonesome."

str_vectorized = count_vectorizer.transform({str})

str_pred = ann.predict(str_vectorized)
str_pred

str_output = np.zeros_like(str_pred)
str_output[np.arange(len(str_pred)), str_pred.argmax(1)] = 1

str_output

print(str_output.shape)

probs = output_probs(reverse_mappings, str_pred)
probs

prediction = output_probs(reverse_mappings, str_output)
max(prediction, key=prediction.get)

